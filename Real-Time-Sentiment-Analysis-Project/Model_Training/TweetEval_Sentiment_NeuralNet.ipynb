{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedfeffc-b87f-42fa-9c86-86379a89a117",
   "metadata": {},
   "source": [
    "### Real-Time Sentiment Analysis for Customer Feedback Using Neural Networks and Streamlit App\n",
    "\n",
    "\n",
    "**To Develop a system that uses a Neural Network (NN) model to perform sentiment analysis on customer feedback provided through a web application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada25b0-6811-4b74-acdb-79f51597de80",
   "metadata": {},
   "source": [
    "#### Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a01ad6a-13e6-48e2-af02-67d8490859bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face dataset loader\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e194da5-9fbf-42d2-b1ca-932b36ac0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TweetEval Sentiment Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65076279-ca41-4f75-b2df-c699e1b7ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_val = dataset[\"validation\"].to_pandas()\n",
    "df_test = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ddf838-034f-434e-a1c8-baad284bda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numerical labels to text\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "df_train[\"sentiment\"] = df_train[\"label\"].map(label_map)\n",
    "df_val[\"sentiment\"] = df_val[\"label\"].map(label_map)\n",
    "df_test[\"sentiment\"] = df_test[\"label\"].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd3e142-9562-4b17-889a-38feb0363102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT @user In the original draft of the 7th boo...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label sentiment\n",
       "0  \"QT @user In the original draft of the 7th boo...      2  Positive\n",
       "1  \"Ben Smith / Smith (concussion) remains out of...      1   Neutral\n",
       "2  Sorry bout the stream last night I crashed out...      1   Neutral\n",
       "3  Chase Headley's RBI double in the 8th inning o...      1   Neutral\n",
       "4  @user Alciato: Bee will invest 150 million in ...      2  Positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View sample data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43df361-8daa-4a16-a9a0-21832808c8db",
   "metadata": {},
   "source": [
    "# 1. LSTM-BASED SENTIMENT CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1237d3-8b23-4f22-b34c-79e88d984cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680e18d0-ba46-49a7-adbb-86d676faaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Pad the text\n",
    "# Parameters\n",
    "vocab_size = 20000\n",
    "max_len = 100\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_train['text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train['text'])\n",
    "X_val = tokenizer.texts_to_sequences(df_val['text'])\n",
    "X_test = tokenizer.texts_to_sequences(df_test['text'])\n",
    "\n",
    "# Padding\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
    "X_val = pad_sequences(X_val, maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "\n",
    "y_train = df_train['label']\n",
    "y_val = df_val['label']\n",
    "y_test = df_test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771268fb-b642-486c-b2a6-2608cc4c2222",
   "metadata": {},
   "source": [
    "**Checking Class Distribution (for class weights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccaff66b-94ec-40be-b8eb-2a82182f9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 2.14366276610743, 1: 0.7355004111643206, 2: 0.8518684520141184}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights for imbalance handling\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27024b27-bec0-49ed-8d6a-62fe31e4e2f2",
   "metadata": {},
   "source": [
    "**LSTM Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1fca6a-ce7e-4300-8391-645eee637560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,741,187</span> (10.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,741,187\u001b[0m (10.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,741,187</span> (10.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,741,187\u001b[0m (10.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lstm.build(input_shape=(None, max_len))\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "473585de-07c0-4c3b-82a7-3929898b619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 50) (45615,)\n",
      "(2000, 50) (2000,)\n",
      "(12284, 50) (12284,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)\n",
    "print(X_val.shape,y_val.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a67a5e-0d05-491e-8c0d-067b4cd12f5d",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56719d6-a7c9-4ebf-893a-f54910417fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 57ms/step - accuracy: 0.3232 - loss: 1.1005 - val_accuracy: 0.1560 - val_loss: 1.1029\n",
      "Epoch 2/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 57ms/step - accuracy: 0.3621 - loss: 1.0825 - val_accuracy: 0.4345 - val_loss: 1.0927\n",
      "Epoch 3/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 57ms/step - accuracy: 0.3118 - loss: 1.1011 - val_accuracy: 0.1560 - val_loss: 1.0995\n",
      "Epoch 4/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 56ms/step - accuracy: 0.3110 - loss: 1.0983 - val_accuracy: 0.4095 - val_loss: 1.0959\n",
      "Epoch 5/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 57ms/step - accuracy: 0.2326 - loss: 1.1034 - val_accuracy: 0.4095 - val_loss: 1.0868\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(X_train, y_train,\n",
    "                              validation_data=(X_val, y_val),\n",
    "                              epochs=5,\n",
    "                              batch_size=32,\n",
    "                              class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8420d3c-1e9a-415f-9f7f-860975921978",
   "metadata": {},
   "source": [
    "**Evaluation on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "143ef0ba-6c9a-463c-8029-8980f0950d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00      3972\n",
      "     Neutral       0.00      0.00      0.00      5937\n",
      "    Positive       0.19      1.00      0.32      2375\n",
      "\n",
      "    accuracy                           0.19     12284\n",
      "   macro avg       0.06      0.33      0.11     12284\n",
      "weighted avg       0.04      0.19      0.06     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_lstm.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce64ee-84d9-423e-b58e-6331516f5448",
   "metadata": {},
   "source": [
    "**The LSTM Model is Severely Underperforming.**\n",
    "\n",
    "Accuracy: 19%\n",
    "\n",
    "Only predicts \"Positive\" for every input \n",
    "\n",
    "No \"Negative\" or \"Neutral\" class predictions at all\n",
    "\n",
    "This often happens when the model learns a bias toward the majority class or a single class, especially when:\n",
    "\n",
    "There’s class imbalance\n",
    "\n",
    "Model is not generalizing due to lack of semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d2f4e-9cfb-4b8e-b796-ba70c309eec3",
   "metadata": {},
   "source": [
    "**As the LSTM model is not performing well, let's use the pre-trained transformer model BERT for fine-tuning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ca618-592f-4d66-9377-e4b13d244194",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d4b1c-d746-4851-8bc8-693a4e114756",
   "metadata": {},
   "source": [
    "#### BERT-Based Sentiment Classifier (with Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62bbbf3-811f-4eb8-91be-5d2fa31b3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\bertenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "C:\\ProgramData\\anaconda3\\envs\\bertenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Load Tokenizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ed19e-4f72-4863-ad74-34f730ee4f08",
   "metadata": {},
   "source": [
    "**Encode the Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd317eea-0cb6-42d2-bee5-98eca54497c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and truncate/pad to max length\n",
    "def encode_texts(texts, labels):\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='tf')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        labels\n",
    "    ))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = encode_texts(df_train['text'], df_train['label']).batch(16)\n",
    "val_dataset = encode_texts(df_val['text'], df_val['label']).batch(16)\n",
    "test_dataset = encode_texts(df_test['text'], df_test['label']).batch(16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c4917-d899-4172-975b-8b2b8f7a1d6a",
   "metadata": {},
   "source": [
    "**Load and Compile the BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bba02b8-5e04-4928-a819-d12d7890f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\bertenv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\ProgramData\\anaconda3\\envs\\bertenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model_bert.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32350b-4c3f-444f-b5d8-6f81ff7b3c28",
   "metadata": {},
   "source": [
    "**Train the BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8131ffc8-2137-4471-9ea1-5611b1ef3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1847s 18s/step - loss: 0.8210 - accuracy: 0.6169 - val_loss: 0.8467 - val_accuracy: 0.6042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b22c09c3d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert.fit(train_dataset.take(100), validation_data=val_dataset.take(30), epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054a2ff-2c21-417c-97f4-11a47f4b8c90",
   "metadata": {},
   "source": [
    "**Evaluate on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67e1cee5-647f-4e94-9380-5e46d5ce3579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 2085s 3s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.58      0.86      0.69      3972\n",
      "     Neutral       0.82      0.34      0.48      5937\n",
      "    Positive       0.50      0.82      0.62      2375\n",
      "\n",
      "    accuracy                           0.60     12284\n",
      "   macro avg       0.63      0.67      0.60     12284\n",
      "weighted avg       0.68      0.60      0.58     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = model_bert.predict(test_dataset).logits\n",
    "y_pred = tf.argmax(logits, axis=1).numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test['label'], y_pred, target_names=['Negative', 'Neutral', 'Positive']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
