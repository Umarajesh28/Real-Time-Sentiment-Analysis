{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedfeffc-b87f-42fa-9c86-86379a89a117",
   "metadata": {},
   "source": [
    "### Real-Time Sentiment Analysis for Customer Feedback Using Neural Networks and Streamlit App\n",
    "\n",
    "\n",
    "**To Develop a system that uses a Neural Network (NN) model to perform sentiment analysis on customer feedback provided through a web application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada25b0-6811-4b74-acdb-79f51597de80",
   "metadata": {},
   "source": [
    "#### Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a01ad6a-13e6-48e2-af02-67d8490859bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face dataset loader\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef0a25-e7d4-482c-8262-bb130b643868",
   "metadata": {},
   "source": [
    "* Installed the Hugging Face datasets library\n",
    "* The library is essential for loading popular datasets, including TweetEval, which is used for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81825fd3-83b7-453d-982b-490dd2193f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove emojis and special characters (optional)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210c6a1-3e68-47bb-836d-4281f5434e92",
   "metadata": {},
   "source": [
    "This function performs preprocessing on tweet text to prepare it for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e194da5-9fbf-42d2-b1ca-932b36ac0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TweetEval Sentiment Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = dataset[split].map(lambda x: {\"text\": preprocess_text(x[\"text\"])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48830c-4d69-439a-a4af-0d7b7a339bda",
   "metadata": {},
   "source": [
    "* Loaded the TweetEval dataset using Hugging Face's datasets library.\n",
    "\n",
    "* tweet_eval is a benchmark dataset for sentiment classification (labels: 0=Negative, 1=Neutral, 2=Positive).\n",
    "\n",
    "* Applied preprocessing to train, validation and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65076279-ca41-4f75-b2df-c699e1b7ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_val = dataset[\"validation\"].to_pandas()\n",
    "df_test = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6cbe5-6fa4-4576-97de-edc961ca555d",
   "metadata": {},
   "source": [
    "* Converted the dataset splits (train, validation, test) to pandas DataFrames for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ddf838-034f-434e-a1c8-baad284bda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numerical labels to text\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "df_train[\"sentiment\"] = df_train[\"label\"].map(label_map)\n",
    "df_val[\"sentiment\"] = df_val[\"label\"].map(label_map)\n",
    "df_test[\"sentiment\"] = df_test[\"label\"].map(label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919a466-37b1-4172-b507-cf4e622e245b",
   "metadata": {},
   "source": [
    "* Adds a new sentiment column with human-readable labels using a mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd3e142-9562-4b17-889a-38feb0363102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qt in the original draft of the 7th book remus...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ben smith smith concussion remains out of the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sorry bout the stream last night i crashed out...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chase headleys rbi double in the 8th inning of...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alciato bee will invest 150 million in january...</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label sentiment\n",
       "0  qt in the original draft of the 7th book remus...      2  Positive\n",
       "1  ben smith smith concussion remains out of the ...      1   Neutral\n",
       "2  sorry bout the stream last night i crashed out...      1   Neutral\n",
       "3  chase headleys rbi double in the 8th inning of...      1   Neutral\n",
       "4  alciato bee will invest 150 million in january...      2  Positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View sample data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf593d-69f9-4886-8f94-b86933212986",
   "metadata": {},
   "source": [
    "* Displays the first few rows of the training dataset to verify structure and content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43df361-8daa-4a16-a9a0-21832808c8db",
   "metadata": {},
   "source": [
    "### 1. LSTM-BASED SENTIMENT CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b1237d3-8b23-4f22-b34c-79e88d984cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72a095-fd89-4fb8-8e6a-7dd9903425ce",
   "metadata": {},
   "source": [
    "* Loads the libraries needed for:\n",
    "\n",
    "  * Text tokenization & padding (Tokenizer, pad_sequences)\n",
    "\n",
    "  * LSTM model building (Sequential, Embedding, LSTM, Dense, Dropout)\n",
    "\n",
    "  * Evaluation metrics (classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "680e18d0-ba46-49a7-adbb-86d676faaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Pad the text\n",
    "# Parameters\n",
    "vocab_size = 20000\n",
    "max_len = 100\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_train['text'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train['text'])\n",
    "X_val = tokenizer.texts_to_sequences(df_val['text'])\n",
    "X_test = tokenizer.texts_to_sequences(df_test['text'])\n",
    "\n",
    "# Padding\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
    "X_val = pad_sequences(X_val, maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "\n",
    "y_train = df_train['label']\n",
    "y_val = df_val['label']\n",
    "y_test = df_test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad250b60-8291-4b80-b2fc-b4959d0cd357",
   "metadata": {},
   "source": [
    "* Tokenization: Converts text into sequences of integers.\n",
    "\n",
    "* Padding: Ensures all sequences are the same length (100) for model input.\n",
    "\n",
    "* <OOV> token: Handles out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771268fb-b642-486c-b2a6-2608cc4c2222",
   "metadata": {},
   "source": [
    "**Checking Class Distribution (for class weights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccaff66b-94ec-40be-b8eb-2a82182f9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: np.float64(2.14366276610743), 1: np.float64(0.7355004111643206), 2: np.float64(0.8518684520141184)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights for imbalance handling\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27024b27-bec0-49ed-8d6a-62fe31e4e2f2",
   "metadata": {},
   "source": [
    "**LSTM Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1fca6a-ce7e-4300-8391-645eee637560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)           </span>┃<span style=\"font-weight: bold\"> Output Shape    </span>┃<span style=\"font-weight: bold\">   Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "│                        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "│                        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└────────────────────────┴─────────────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,     │ \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "│                        │ \u001b[38;5;34m128\u001b[0m)            │           │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,     │   \u001b[38;5;34m131,584\u001b[0m │\n",
       "│                        │ \u001b[38;5;34m128\u001b[0m)            │           │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,     │         \u001b[38;5;34m0\u001b[0m │\n",
       "│                        │ \u001b[38;5;34m128\u001b[0m)            │           │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │    \u001b[38;5;34m49,408\u001b[0m │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │         \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────┼─────────────────┼───────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │       \u001b[38;5;34m195\u001b[0m │\n",
       "└────────────────────────┴─────────────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,741,187</span> (10.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,741,187\u001b[0m (10.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,741,187</span> (10.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,741,187\u001b[0m (10.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lstm.build(input_shape=(None, max_len))\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97825710-4cac-489f-a0c2-2b20a5e23538",
   "metadata": {},
   "source": [
    "* Embedding Layer: Converts word indices to dense vectors of fixed size (output_dim=128).\n",
    "\n",
    "* LSTM Layer: A Long Short-Term Memory layer with 64 units, useful for sequential dependencies.\n",
    "\n",
    "* Dropout: Prevents overfitting by randomly disabling 50% of the neurons during training.\n",
    "\n",
    "* Dense Output Layer: 3 output units for the 3 sentiment classes with softmax activation.\n",
    "\n",
    "* Loss: sparse_categorical_crossentropy used for multi-class classification with integer labels.\n",
    "\n",
    "* Optimizer: Adam is used for efficient training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473585de-07c0-4c3b-82a7-3929898b619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 100) (45615,)\n",
      "(2000, 100) (2000,)\n",
      "(12284, 100) (12284,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)\n",
    "print(X_val.shape,y_val.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a67a5e-0d05-491e-8c0d-067b4cd12f5d",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c56719d6-a7c9-4ebf-893a-f54910417fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 379ms/step - accuracy: 0.3270 - loss: 1.1007 - val_accuracy: 0.4345 - val_loss: 1.0915\n",
      "Epoch 2/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 403ms/step - accuracy: 0.3388 - loss: 1.1016 - val_accuracy: 0.1560 - val_loss: 1.1044\n",
      "Epoch 3/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 403ms/step - accuracy: 0.3111 - loss: 1.1028 - val_accuracy: 0.4345 - val_loss: 1.0959\n",
      "Epoch 4/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 413ms/step - accuracy: 0.3698 - loss: 1.0991 - val_accuracy: 0.4095 - val_loss: 1.1021\n",
      "Epoch 5/5\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 457ms/step - accuracy: 0.2628 - loss: 1.1038 - val_accuracy: 0.4345 - val_loss: 1.0962\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(X_train, y_train,\n",
    "                              validation_data=(X_val, y_val),\n",
    "                              epochs=5,\n",
    "                              batch_size=32,\n",
    "                              class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad199675-42aa-4b30-8c4a-54c707434fca",
   "metadata": {},
   "source": [
    "* Training for 5 epochs using a batch size of 32.\n",
    "\n",
    "* Validation data is used to evaluate model performance after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8420d3c-1e9a-415f-9f7f-860975921978",
   "metadata": {},
   "source": [
    "**Evaluation on Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143ef0ba-6c9a-463c-8029-8980f0950d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 150ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00      3972\n",
      "     Neutral       0.48      1.00      0.65      5937\n",
      "    Positive       0.00      0.00      0.00      2375\n",
      "\n",
      "    accuracy                           0.48     12284\n",
      "   macro avg       0.16      0.33      0.22     12284\n",
      "weighted avg       0.23      0.48      0.31     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_lstm.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0317e4-15cc-47e8-bc80-3b36f49f8e68",
   "metadata": {},
   "source": [
    "* Model prediction on test data.\n",
    "\n",
    "* np.argmax() gets the predicted class labels.\n",
    "\n",
    "* classification_report() shows precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce64ee-84d9-423e-b58e-6331516f5448",
   "metadata": {},
   "source": [
    "**The LSTM Model is Severely Underperforming.**\n",
    "\n",
    "* **Accuracy = 48%**\n",
    "\n",
    "* The model **predicts only the \"Neutral\" class** (label 1), for all inputs.\n",
    "\n",
    "* **Precision/Recall/F1 for Negative and Positive = 0.00** → they’re not being predicted at all.\n",
    "\n",
    "* **Macro Avg F1 = 0.22** → very poor overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d2f4e-9cfb-4b8e-b796-ba70c309eec3",
   "metadata": {},
   "source": [
    "### LSTM Model Limitations & Observations\n",
    "\n",
    "Although I applied text preprocessing and class weighting to reduce the impact of class imbalance, the LSTM model struggled to learn meaningful patterns. During evaluation, the model predicted almost all test samples as Neutral, resulting in poor recall and F1-score for both Positive and Negative classes.\n",
    "\n",
    "This may be due to:\n",
    "- Class imbalance in the TweetEval dataset\n",
    "- LSTM's limited ability to capture context in short, noisy tweet data\n",
    "- Shallow architecture or limited training time (epochs)\n",
    "\n",
    "To address this, I used a transformer-based **BERT model (`bert-base-uncased`)** fine-tuned on the same dataset. BERT can capture bidirectional context, handle slang and informal language better, and significantly outperforms LSTM on sentiment classification tasks. The BERT model showed improved accuracy and a more balanced prediction across classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d4b1c-d746-4851-8bc8-693a4e114756",
   "metadata": {},
   "source": [
    "#### BERT-Based Sentiment Classifier (with Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62bbbf3-811f-4eb8-91be-5d2fa31b3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\tf-bert\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Load Tokenizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cc5cd-d8a2-42e7-9e27-c639c8744329",
   "metadata": {},
   "source": [
    "* Loads the **pretrained BERT tokenizer**.\n",
    "\n",
    "* bert-base-uncased means all input is lowercased (better for general sentiment tasks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ed19e-4f72-4863-ad74-34f730ee4f08",
   "metadata": {},
   "source": [
    "**Encode the Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd317eea-0cb6-42d2-bee5-98eca54497c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and truncate/pad to max length\n",
    "def encode_texts(texts, labels):\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='tf')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        labels\n",
    "    ))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = encode_texts(df_train['text'], df_train['label']).batch(16)\n",
    "val_dataset = encode_texts(df_val['text'], df_val['label']).batch(16)\n",
    "test_dataset = encode_texts(df_test['text'], df_test['label']).batch(16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee63b5-f99c-424d-a2a1-a699b56db553",
   "metadata": {},
   "source": [
    "* Converts texts into token ID sequences with attention masks.\n",
    "\n",
    "* Creates TensorFlow datasets for train, val, and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c4917-d899-4172-975b-8b2b8f7a1d6a",
   "metadata": {},
   "source": [
    "**Load and Compile the BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bba02b8-5e04-4928-a819-d12d7890f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "100/100 [==============================] - 420s 4s/step - loss: 0.9333 - accuracy: 0.5325 - val_loss: 0.7885 - val_accuracy: 0.6292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x107f42ce050>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model_bert.summary()\n",
    "\n",
    "model_bert.fit(train_dataset.take(100), validation_data=val_dataset.take(30), epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176cd96-9889-43b3-a57d-33b4c8761664",
   "metadata": {},
   "source": [
    "* Loads BERT with a classification head for 3 labels: Negative, Neutral, and Positive.\n",
    "  \n",
    "* Uses a low learning rate (5e-5), ideal for fine-tuning BERT.\n",
    "\n",
    "* from_logits=True is important as BERT outputs raw scores (logits)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0529f-1a67-4ab8-a1ac-4d3212628cd7",
   "metadata": {},
   "source": [
    "**Predict on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3a1a78-0da8-4643-a45f-29b2ca9efe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 812s 1s/step\n"
     ]
    }
   ],
   "source": [
    "logits = model_bert.predict(test_dataset).logits\n",
    "y_pred = tf.argmax(logits, axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b5e75-3097-4fb1-8c44-eac1a8f678f8",
   "metadata": {},
   "source": [
    "* Generates logits for each test sample and converts to class predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054a2ff-2c21-417c-97f4-11a47f4b8c90",
   "metadata": {},
   "source": [
    "**Evaluate on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67e1cee5-647f-4e94-9380-5e46d5ce3579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.61      0.79      0.69      3972\n",
      "     Neutral       0.72      0.58      0.64      5937\n",
      "    Positive       0.62      0.61      0.62      2375\n",
      "\n",
      "    accuracy                           0.65     12284\n",
      "   macro avg       0.65      0.66      0.65     12284\n",
      "weighted avg       0.66      0.65      0.65     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test['label'], y_pred, target_names=['Negative', 'Neutral', 'Positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475bd5d-e68c-4e72-ab15-455b03710ccf",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "  \n",
    "* Accuracy improved to 65% (vs 48% with LSTM).\n",
    "\n",
    "* Recall improved across all classes.\n",
    "\n",
    "* The model is much more balanced in its predictions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-bert)",
   "language": "python",
   "name": "tf-bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
